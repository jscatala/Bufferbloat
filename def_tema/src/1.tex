If a little salt makes food taste better, then a lot must make it taste great, right. What happend if you apply the same statement to a network domain? It keeps been as good as it was? It improves the performance or makes it worse?.\\

\textit{Lets think of a network as a road system where everyone drives at the maximum speed. When the road gets full, there are only two choices: crash into other cars, or get off the road and wait until things get better. The former isn't as disastrous on a network as it would be in real life: losing packets in the middle of a communication session isn't a big deal. But making a packet wait for a short time is usually better than ``dropping" it and having to wait for a retransmission.}[iv]\\

At this point, the role of the router becomes important. It has to control the congestion effectively in networks. It is important to remember that the traffic in a network is inherently bursty, so the role of the buffers in the router is to smooth the flow of traffic. Without any buffering, to allocate the bandwidth evenly would be impossible. But there are some problems with current algorithms; they use tail-drop based queue management that has two big drawbacks: 1.- lockout 2.- full queue that impact with a high queue delay.\\

These problems are fixed with the creation of a group of FIFO based queue management mechanisms to support end-to-end congestion control in the internet. That procedure is called Active Queue Management (AQM). With AQM the loss of package and the average queue length is reduced; this impacts in a decreasing end-to-end delay by droping packages before buffer comes full, using the exponential weighted average queue length as a congestion indicator. For the proper use of AQM, it has to be widely enabled and consistently configured the router.\\

\textit{Today's networks are suffering from unnecessary latency and poor system performance. The culprit is Bufferbloat, the existence of excessively large and frequently full buffers inside the network. Large buffers have been inserted all over the Internet without sufficient thought or testing. They damage or defeat the fundamental congestion-avoidance algorithms of the Internet's most common transport protocol. Long delays from bufferbloat are frequently attributed incorrectly to network congestion, and this misinterpretation of the problem leads to the wrong solutions being proposed.}[i]\\

The existence of cheap memory and a misguided desire to avoid packet loss has led to larger and larger buffers being deployed in the hosts, routers, and switches that make up the Internet. It turns out that this is a recipe for bufferbloat. Evidence of bufferbloat has been accumulating over the past decade, but its existence has not yet become a widespread cause for concern.\\

This phenomenon lately has begun to being a concern and you can find in websites like \url{http://lwn.net}, publications as \url{http://queue.acm.org} or with a simple google' search many hits and mail lists related that threats the subject. All this let to the creation of a specific website [v], where anyone who is intrested in the topics related with TCP/IP networks but mostly with \textit{Bufferbloat} can join. All of these work, led us to the work of whose been the leader and the one who show the importance that this phenomenon really has; Jim Gettys and his personal blog[iii], subject that this thesis is based on.
