Unmanaged buffers are more critical today since buffers sizes are larger,
delay-sensitive applications are more prevalent, and large downloads common
but overly large, unmanaged, and uncoordinated buffers create excessive delays
that frustrate and baffle end users.

Correct buffer sizing is not an easy problem. Undersizing - making buffers
smaller than the traditional BDP- is fraught with problems. Today's links vary
in bandwidth, and individual connections vary in RTT. This makes it impossible
to properly pick a static size for most edge links. A simple, robust algorithm
that can manage buffer delay regardless of buffer size and link bandwidth
without a negative impact on utilization can make over-sized buffers
irrelevant.

A widely used rule-of-thumb recommends the buffers of size $B = \overline{RTT}
x C $, where $\overline{RTT}$ is the average round trip time experienced by
connections utilizing the link, and C is the data rate of the link. Many
buffers have been inserted without sufficient care due to the complexity to
establish the appropriate size and test on real environments\cite{Vu-Brugier},
helping to determine where further complicate occurs Bufferbloat.

While the rule-of-thumb comes from the idea of ​​maintaining the medium as
busy as possible, and to maximize the throughput of the medium. But due to the
characteristics of TCP, no matter how large the buffer is, it will always be
saturated.

The main condition to choose the proper size of a buffer is the ability to
keep sending data in the periods in which the sender is stopped, preventing
possible downtimes in order to maximize utilization of the link and kept high
throughput at all times. The buffer will avoid to idle if the first packet
from the sender shows up at the buffer just as it hits empty.

After a lost is detected, the cwnd is set to half of is last value, so if we
denote as $(W_{max} /2)/C$ the amount of time that packets are sent in
congestion phase, and as $B/C$ the time that takes a buffer with size B to
drain, the size of a buffer B needed is $B \leq (W_{max} /2)$
\cite{main:ref:1}.

The problem occurs when the required size buffers are implemented, and the
size is exceeded producing overbuffering. Overbuffering hurts anytime a link
is saturated causing extra delay, and destroying many uses of moder Internet.
As stated in \cite{GettysNichols}, \textit{a TCP connection must react quickly
to changes in demand at the bottleneck link, but TCP reaction time is
quadratic to the amount of overbuffering}.

Unfortunately this error comes from the fact that manufacturers began to think
that adding more buffer to their products should bring positive consequences,
especially considering the current volumes of data, in addition to providing
more equitable access to available bandwidth handled.